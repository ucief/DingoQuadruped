{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1Xde03mrZv8"
      },
      "source": [
        "## Local Setup\n",
        "\n",
        "If you prefer to work locally, see the following instructions for setting up Python in a virtual environment.\n",
        "You can then ignore the instructions in \"Colab Setup\".\n",
        "\n",
        "If you haven't yet, create a [conda](https://docs.conda.io/projects/conda/en/latest/user-guide/install/index.html) environment using:\n",
        "```\n",
        "conda create --name rl_exercises\n",
        "conda activate rl_exercises\n",
        "```\n",
        "Torch recommends installation using conda rather than pip, so run e.g.:\n",
        "```\n",
        "conda install pytorch pytorch-cuda=11.8 -c pytorch -c nvidia\n",
        "```\n",
        "For this exercise, you require a CUDA-enabled GPU, as training an image-based model on the CPU takes a very long time.\n",
        "Visit [the installation page](https://pytorch.org/get-started/locally/) to see the options available for different CUDA versions.\n",
        "The remaining dependencies can be installed with pip:\n",
        "```\n",
        "pip install matplotlib numpy tqdm ipykernel \"gymnasium[classic-control, other]\" scikit-learn\n",
        "```\n",
        "\n",
        "Even if you are running the Jupyter notebook locally, please run the code cells in **Colab Setup**, because they define some global variables required later."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sxoAbnmrZv-"
      },
      "source": [
        "## Colab Setup\n",
        "\n",
        "Google Colab provides you with a temporary environment for python programming.\n",
        "While this conveniently works on any platform and internally handles dependency issues and such, it also requires you to set up the environment from scratch every time.\n",
        "The \"Colab Setup\" section below will be part of **every** exercise and contains utility that is needed before getting started.\n",
        "\n",
        "**IMPORTANT**: For this exercise, you require a GPU runtime environment, as training an image-based model on the CPU takes a very long time.\n",
        "To do this, select \"Change runtime type\" from the context menu in the top right corner (next to the **Connect** button), and select **T4 GPU**.\n",
        "\n",
        "There is a timeout of about ~12 hours with Colab while it is active (and less if you close your browser window).\n",
        "Any changes you make to the Jupyter notebook itself should be saved to your Google Drive.\n",
        "We also save all recordings and logs in it by default so that you won't lose your work in the event of an instance timeout.\n",
        "However, you will need to re-mount your Google Drive and re-install packages with every new instance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fE1euUt1rZv_",
        "outputId": "e38b7197-dd66-42d9-d70e-8a044f630398",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "/content/gdrive/My Drive/rl_ws23\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n",
            "Collecting gymnasium[accept-rom-license,atari,classic-control,other]\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.47.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari,classic-control,other]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari,classic-control,other]) (4.5.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium[accept-rom-license,atari,classic-control,other])\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari,classic-control,other]) (2.5.2)\n",
            "Collecting lz4>=3.1.0 (from gymnasium[accept-rom-license,atari,classic-control,other])\n",
            "  Downloading lz4-4.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opencv-python>=3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari,classic-control,other]) (4.8.0.76)\n",
            "Requirement already satisfied: moviepy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari,classic-control,other]) (1.0.3)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari,classic-control,other]) (2.1.0+cu121)\n",
            "Collecting autorom[accept-rom-license]~=0.4.2 (from gymnasium[accept-rom-license,atari,classic-control,other])\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Collecting shimmy[atari]<1.0,>=0.1.0 (from gymnasium[accept-rom-license,atari,classic-control,other])\n",
            "  Downloading Shimmy-0.2.1-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari,classic-control,other]) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari,classic-control,other]) (2.31.0)\n",
            "Collecting AutoROM.accept-rom-license (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari,classic-control,other])\n",
            "  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.10/dist-packages (from moviepy>=1.0.0->gymnasium[accept-rom-license,atari,classic-control,other]) (4.4.2)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.10/dist-packages (from moviepy>=1.0.0->gymnasium[accept-rom-license,atari,classic-control,other]) (0.1.10)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.10/dist-packages (from moviepy>=1.0.0->gymnasium[accept-rom-license,atari,classic-control,other]) (2.31.6)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from moviepy>=1.0.0->gymnasium[accept-rom-license,atari,classic-control,other]) (0.4.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Collecting ale-py~=0.8.1 (from shimmy[atari]<1.0,>=0.1.0->gymnasium[accept-rom-license,atari,classic-control,other])\n",
            "  Downloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m73.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->gymnasium[accept-rom-license,atari,classic-control,other]) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->gymnasium[accept-rom-license,atari,classic-control,other]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->gymnasium[accept-rom-license,atari,classic-control,other]) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->gymnasium[accept-rom-license,atari,classic-control,other]) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->gymnasium[accept-rom-license,atari,classic-control,other]) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->gymnasium[accept-rom-license,atari,classic-control,other]) (2.1.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.1->shimmy[atari]<1.0,>=0.1.0->gymnasium[accept-rom-license,atari,classic-control,other]) (6.1.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from imageio-ffmpeg>=0.2.0->moviepy>=1.0.0->gymnasium[accept-rom-license,atari,classic-control,other]) (67.7.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari,classic-control,other]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari,classic-control,other]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari,classic-control,other]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari,classic-control,other]) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->gymnasium[accept-rom-license,atari,classic-control,other]) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.0.0->gymnasium[accept-rom-license,atari,classic-control,other]) (1.3.0)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.6.1-py3-none-any.whl size=446660 sha256=88a0ea299288202ca52e1b765998964f0efe5a1e4c702be04eebde915866fa10\n",
            "  Stored in directory: /root/.cache/pip/wheels/6b/1b/ef/a43ff1a2f1736d5711faa1ba4c1f61be1131b8899e6a057811\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: farama-notifications, lz4, gymnasium, ale-py, shimmy, AutoROM.accept-rom-license, autorom\n",
            "Successfully installed AutoROM.accept-rom-license-0.6.1 ale-py-0.8.1 autorom-0.4.2 farama-notifications-0.0.4 gymnasium-0.29.1 lz4-4.3.3 shimmy-0.2.1\n"
          ]
        }
      ],
      "source": [
        "\"\"\"Your work will be stored in a folder called `rl_ws23` by default to prevent Colab\n",
        "instance timeouts from deleting your edits.\n",
        "We do this by mounting your google drive on the virtual machine created in this colab\n",
        "session. For this, you will likely need to sign in to your Google account and allow\n",
        "access to your Google Drive files.\n",
        "\"\"\"\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "try:\n",
        "    from google.colab import drive\n",
        "\n",
        "    drive.mount(\"/content/gdrive\")\n",
        "    COLAB = True\n",
        "except ImportError:\n",
        "    COLAB = False\n",
        "\n",
        "# Create paths in your google drive\n",
        "if COLAB:\n",
        "    DATA_ROOT = Path(\"/content/gdrive/My Drive/rl_ws23\")\n",
        "    DATA_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    DATA_ROOT_STR = str(DATA_ROOT)\n",
        "    %cd \"$DATA_ROOT\"\n",
        "else:\n",
        "    DATA_ROOT = Path.cwd() / \"rl_ws23\"\n",
        "\n",
        "# Install python packages\n",
        "if COLAB:\n",
        "    %pip install matplotlib numpy tqdm \"gymnasium[atari, accept-rom-license, classic-control, other]\"  scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UunygyDXrx7k"
      },
      "source": [
        "# Exercise 4 - Model-Based Reinforcement Learning\n",
        "\n",
        "Designed by Vaisakh Shaj and Niklas Freymuth.\n",
        "\n",
        "In this homework, we are going to implement a simple model predictive control (MPC) algorithm using a learned dynamics and reward model.\n",
        "\n",
        "### Principle\n",
        "We consider the optimal control problem of an MDP with an **unknown deterministic** reward function $r$ and subject to **unknown deterministic** dynamics $s_{t+1} = f(s_t, a_t)$:\n",
        "\n",
        "$$\\max_{(a_0,a_1,\\dotsc)} \\sum_{t=0}^\\infty \\gamma^t r(s_t,a_t)$$\n",
        "\n",
        "In **model-based reinforcement learning**, this problem is solved in **two steps**:\n",
        "1. **Model learning**:\n",
        "We learn a model of the dynamics $f_\\theta \\simeq f$ and reward function $r_\\theta \\simeq r$ through regression on interaction data.\n",
        "2. **Planning**:\n",
        "We leverage the dynamics model $f_\\theta$ to compute the optimal trajectory $$\\max_{(a_0,a_1,\\dotsc)} \\sum_{t=0}^\\infty \\gamma^t r_{\\theta}(\\hat{s}_t,a_t)$$ following the learnt dynamics $\\hat{s}_{t+1} = f_\\theta(\\hat{s}_t, a_t)$.\n",
        "\n",
        "(We can easily extend to stochastic dynamics, but we consider the simpler case in this homework)\n",
        "\n",
        "\n",
        "In this homework you will implement the model-based algorithm proposed in section IV of [this paper](https://arxiv.org/abs/1708.02596) with some differences:\n",
        "\n",
        "1. along with the next environment state, also the reward is learned. To do that another neural network has been used.\n",
        "2. We train on the pybullet gym environment of inverted pendulum.\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "&\\overline{\\text { Algorithm } \\mathbf{1} \\text { Model-based Reinforcement Learning }} \\\\\n",
        "&\\hline \\text { 1: gather dataset } \\mathcal{D}_{\\text {RAND }} \\text { of random trajectories } \\\\\n",
        "&\\text { 2: initialize empty dataset } \\mathcal{D}_{\\text {RL }} \\text {, and randomly initialize } f_{\\theta} \\\\\n",
        "&  \\text{ 3: } \\textbf{for} \\text{ iter=1 to max_iter } \\textbf{do}  \\\\\n",
        "&\\quad  \\quad \\quad \\textbf {Model Learning } \\\\\n",
        "&\\text { 4:} \\quad  \\quad \\text{ train } f_{\\theta}(\\mathbf{s}, \\mathbf{a}) \\text{ and } r_{\\theta}(\\mathbf{s}, \\mathbf{a}) \\text { by performing gradient descent on MSE Loss }  \\\\\n",
        "& \\quad \\quad  \\quad \\text { using } \\mathcal{D}_{\\text {RAND }} \\text { and } \\mathcal{D}_{\\text {RL }} \\\\\n",
        "&\\quad  \\quad \\quad \\textbf {Planning and More Experience Collection } \\\\\n",
        "&\\text { 5: } \\quad  \\quad  \\textbf { for } t=1 \\text { to } T \\textbf { do } \\\\\n",
        "&\\text { 6: } \\quad \\quad \\quad \\quad \\text { get agent's current state } \\mathbf{s}_{t} \\\\\n",
        "&\\text { 7: } \\quad \\quad \\quad \\quad \\text { use } f_{\\theta} \\text { and } r_{\\theta} \\text { to estimate optimal action sequence } \\mathbf{A}_{t}^{(H)} \\\\\n",
        "&\\text { 8: } \\quad \\quad \\quad \\quad \\text { execute first action } \\mathbf{a}_{t} \\text { from selected action sequence } \\\\\n",
        "&\\text { 9: } \\quad \\quad \\quad \\quad\\text { Add } \\{s_t,  s_{t+1}, r_t, a_t\\} \\text { to } \\mathcal{D}_{\\text {RL }} \\\\\n",
        "&\\text { 10: } \\quad \\text { end for } \\\\\n",
        "&\\text { 11: end for } \\\\\n",
        "&\\hline\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "You will notice that compared to model free methods, model based methods are more sample efficient, at the cost of a more complex algorithm.\n",
        "\n",
        "### Code Overview\n",
        "\n",
        "The code below is organized into the following blocks.\n",
        "The bolded sections require your input:\n",
        " * Import statements and utility functions for plotting\n",
        " * Hyperparameters\n",
        " * **Helper Functions**\n",
        " * The neural network models\n",
        " * A random planner\n",
        " * **A planner based on the cross-entropy method**\n",
        " * The main training loop, which coordinates rollouts, updating the replay buffer, and network updates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "id": "enh5ZMHftEO7"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from typing import Sequence, List\n",
        "\n",
        "import gymnasium as gym\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import tqdm\n",
        "from gymnasium.utils.save_video import save_video\n",
        "from torch import nn, optim\n",
        "import random\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "OUTPUT_FOLDER = DATA_ROOT / \"exercise_4\" / time.strftime(\"%Y-%m-%d_%H-%M\")\n",
        "OUTPUT_FOLDER.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Set random seeds\n",
        "SEED = 123\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "\n",
        "# Required to display matplotlib figures as cell output\n",
        "%matplotlib inline\n",
        "\n",
        "def plot_rewards(\n",
        "        *reward_curves: np.ndarray,\n",
        "        colors: Sequence[str],\n",
        "        labels: Sequence[str],\n",
        "        title: str = \"Policy Performance during Training\",\n",
        "        ylabel: str = \"Reward\",\n",
        ") -> None:\n",
        "    plt.figure()\n",
        "    for reward_curve, color, label in zip(reward_curves, colors, labels):\n",
        "        plt.plot(reward_curve, color=color, label=label)\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Iterations\")\n",
        "    plt.ylabel(ylabel)\n",
        "    plt.legend()\n",
        "\n",
        "\n",
        "def save_current_figure(save_name: str) -> None:\n",
        "    plt.savefig(str(OUTPUT_FOLDER / f\"{save_name}.png\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "n36K8CFIZVYZ"
      },
      "source": [
        "### Set hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "imnAkQ6jryL7"
      },
      "outputs": [],
      "source": [
        "# In Google Colab, this code block renders as a form where hyperparameters can be adjusted.\n",
        "\n",
        "# World model hyperparameters\n",
        "# @markdown Number of random trajectories to collect\n",
        "NUM_RAND_TRAJECTORIES = 10  # @param {type: \"integer\"}\n",
        "# @markdown Dynamic Model learning rate\n",
        "ENV_LEARNING_RATE = 3e-4  # @param {type: \"number\"}\n",
        "# @markdown Reward Model learning rate\n",
        "REW_LEARNING_RATE = 3e-4  # @param {type: \"number\"}\n",
        "# @markdown Number of transitions to sample from the replay buffer for each update:\n",
        "BATCH_SIZE = 2000  # @param {type: \"integer\"}\n",
        "# @markdown Training Iterations:\n",
        "TRAIN_ITER_MODEL = 50  # @param {type: \"integer\"}\n",
        "\n",
        "# Controller hyperparameters\n",
        "# @markdown Prediction horizon of the controller\n",
        "HORIZON_LENGTH = 15  # @param {type: \"integer\"}\n",
        "# @markdown Number of action sequences/trajectories to sample from the controller\n",
        "NUM_ACTIONS_SEQUENCES = 100  # @param {type: \"integer\"}\n",
        "\n",
        "# Training hyperparameters\n",
        "# @markdown Number of outer training iterations\n",
        "NUM_ITERATIONS = 100  # @param {type: \"integer\"}\n",
        "# @markdown Number of steps/inner iterations\n",
        "NUM_STEPS = 200  # @param {type: \"integer\"}\n",
        "# @markdown The planner to use. \"cem\" or \"random\"\n",
        "PLANNER = \"cem\"  # @param [\"cem\", \"random\"] {type: \"string\"}\n",
        "# @markdown The number of iterations to optimize the cross entropy method for\n",
        "CEM_OPTIMIZATION_ITERS = 5  # @param {type: \"integer\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "v3l3mzBjZVYZ"
      },
      "source": [
        "# Collect Random Trajectories To Train A Dynamics Model and Reward Model (Experience Collection)\n",
        "\n",
        "First, we randomly interact with the environment to produce a batch of experiences\n",
        "\n",
        "$$D = \\{s_t,  s_{t+1}, r_t, a_t\\}_{t\\in[1,N]}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "kWuhNkSArL1V"
      },
      "outputs": [],
      "source": [
        "def gather_random_trajectories(env: gym.Env) -> List[List[np.ndarray]]:\n",
        "    \"\"\"\n",
        "    Collect random trajectories from an environment.\n",
        "\n",
        "    This function runs a given number of random trajectories in the specified environment.\n",
        "    It collects data about states, next states, rewards, and actions for each step in the trajectories.\n",
        "    This data can be used to train models in a supervised manner.\n",
        "\n",
        "    Args:\n",
        "        env (gym.Env): The environment to run the trajectories in.\n",
        "\n",
        "    Returns:\n",
        "        list: A list containing the state, next state, reward, and action for each step.\n",
        "\n",
        "    \"\"\"\n",
        "    rewards = []  # To store rewards for each game\n",
        "    observations = []  # To store observations for each game\n",
        "    next_observations = []  # To store next observations for each game\n",
        "    actions = []  # To store actions for each game\n",
        "\n",
        "    for _ in tqdm.tqdm(range(NUM_RAND_TRAJECTORIES), desc=\"Gathering Random Trajectories\"):\n",
        "        obs, _ = env.reset()  # Reset environment to start state\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            sampled_action = env.action_space.sample()  # Sample a random action\n",
        "            new_obs, reward, terminated, truncated, _ = env.step(sampled_action)  # Apply action\n",
        "            done = terminated or truncated\n",
        "\n",
        "            # Append the observed data to the dataset\n",
        "            observations.append(obs)\n",
        "            next_observations.append(new_obs)\n",
        "            rewards.append(reward)\n",
        "            actions.append(sampled_action)\n",
        "\n",
        "            obs = new_obs  # Update the current observation\n",
        "\n",
        "    # create a transposed dataset for easier access\n",
        "    data = [observations, next_observations, rewards, actions]\n",
        "    data = list(map(list, zip(*data)))\n",
        "\n",
        "    # Calculate and print statistics\n",
        "    mean_reward = np.round(np.sum(rewards) / NUM_RAND_TRAJECTORIES, 2)\n",
        "    max_reward = np.round(np.max(rewards), 2)\n",
        "    avg_steps = np.round(len(rewards) / NUM_RAND_TRAJECTORIES)\n",
        "    print(f\"Random trajectory statistics:\\n \"\n",
        "          f\"Mean R: {mean_reward}, Max R: {max_reward}, Avg Steps: {avg_steps}\")\n",
        "    return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "be9mXNKhZVYa"
      },
      "source": [
        "# Deep Dynamics/Reward Models With Feed Forward NNs\n",
        "\n",
        "A dynamics model takes in the state $s_t$ and action $a_t$ at the current time step and predicts the state $s_{t+1}$ at the next time step.  \n",
        "\n",
        "**However this function can be difficult to learn when the states $s_t$ and $s_{t+1}$ are too similar and the action has seemingly littl eeffect on the output; this difficulty becomes more pronouncedas the time between states $\\Delta t$ becomes smaller and the state differences do not indicate the underlying dynamics well. Thus we typically a function that predict the differences to the next state. i.e. $\\Delta s_{t+1} = s_{t+1} - s_t = f(s_t,a_t)$. This allows us to learn more accurate models in practice.**\n",
        "\n",
        "Similarly, a reward model takes in the state $s_t$ and action$a_t$ at the current time step and predicts reward $r_{t}$ that we can obtain based on that action.  \n",
        "\n",
        "In the following block, we define the Feedforward Network. For each instance of this network, the input is a state and action at current time step $s_t$, $a_t$ respectively. The output is a reward for the reward model, and the difference to the next step for the dynamics model.\n",
        "For NN training, we normally feed data in a mini-batch manner. Since each state in current task is a $1$st order image tensor (concatenation of $s_t$ and $a_t$), the model expects input to be a $2$nd order tensor with shape (mini-batch, state_dim + action_dim). And the output is a 2nd order tensor with shape (mini-batch, state_dim)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "id": "fLnU1evmss4I"
      },
      "outputs": [],
      "source": [
        "class FeedforwardModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Model that predict the difference to next state, given the current state and action\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim: int, output_dim: int):\n",
        "        \"\"\"\n",
        "        input_dim: Input dimension. Usually state_dim + action_dim\n",
        "        output_dim: Output dimension. state_dim / reward dim\n",
        "        \"\"\"\n",
        "        super(FeedforwardModel, self).__init__()\n",
        "\n",
        "        # build a multi-layer perceptron\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(input_dim, 512),\n",
        "            nn.BatchNorm1d(num_features=512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.BatchNorm1d(num_features=256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.mlp(x.float())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXPHcsDrZVYa"
      },
      "source": [
        "# Training The Dynamics And Reward Models (3 Pts)\n",
        "We can now train our models $f_\\theta$ and $r_\\theta$ in a supervised fashion to minimize an MSE loss over our experience batch by stochastic gradient descent:\n",
        "\n",
        "$$L_{Dynamics} = \\frac{1}{|D|}\\sum_{s_t,a_t,s_{t+1}\\in D}||\\Delta s_{t+1}- f_\\theta(s_t, a_t)||^2$$\n",
        "\n",
        "$$L_{Reward} = \\frac{1}{|D|}\\sum_{s_t,a_t,r_{t}\\in D}||r_{t}- r_\\theta(s_t, a_t)||^2$$\n",
        "\n",
        "-----\n",
        "In practice, it’s helpful to normalize the target of a neural network.  So in the code, we’ll train the network to predict a normalized version of the change in state, as in\n",
        "\n",
        "$$L_{Dynamics} = \\frac{1}{|D|}\\sum_{s_t,a_t,s_{t+1}\\in D}||\\text{normalize}(\\Delta s_{t+1})- f_\\theta(s_t, a_t)||^2$$\n",
        "\n",
        "Similarly, we’ll train the network to predict a normalized version of the reward, as in\n",
        "\n",
        "$$L_{Reward} = \\frac{1}{|D|}\\sum_{s_t,a_t,r_{t}\\in D}||\\text{normalize}(r_{t})- r_\\theta(s_t, a_t)||^2$$\n",
        "\n",
        "\n",
        "\n",
        "------\n",
        "Since $f_{\\theta}$ is trained to predict the normalized state difference, you generate thenext prediction with\n",
        "$$\n",
        "\\hat{\\mathbf{s}}_{t+1}=\\mathbf{s}_{t}+\\text { Unnormalize }\\left(f_{\\theta}\\left(\\mathbf{s}_{t}, \\mathbf{a}_{t}\\right)\\right)\n",
        "$$ \\\\\n",
        "\n",
        "You generate the reward with $$\n",
        "\\hat{\\mathbf{r}}_{t}=\\text { Unnormalize }\\left(r_{\\theta}\\left(\\mathbf{s}_{t}, \\mathbf{a}_{t}\\right)\\right)\n",
        "$$\n",
        "\n",
        "## Task 1: Training the Dynamics and Reward Models (5 Pts)\n",
        "We will use the mean squared error (MSE) loss as shown in the equations above:\n",
        "- implement the MSE loss in the *model_mse* function (1 pts)\n",
        "\n",
        "To train the dynamics and reward models, we have to properly prepare the training data. Implement the following steps in the *train_dyna_model*\n",
        "- split the dataset into training (80%) and testing (20%) data by converting the dataset into a proper list and slicing it using the *prepare_dataset* function (1 pts)\n",
        "- randomly shuffle the training data (1 pts)\n",
        "- create the input and output data for the dynamics and reward model. Think what the inputs for each of these networks are. Do this procedure for both train and validation data. (2 pts)\n",
        "\n",
        "\n",
        "** The places you have to fill in your code are marked **\n",
        "\n",
        "**Note** We use scikit learn's [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) to perform normalization and unnormalization steps throughout this assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "lj1WrUC9ZVYb"
      },
      "outputs": [],
      "source": [
        "def model_mse(y_true, y_pred, device):\n",
        "    \"\"\"\n",
        "    Compute the MSE (Mean Squared Error) between y_truth and y_pred\n",
        "    Args:\n",
        "        y_true: ground truth values (batch_size x data_dimension)\n",
        "        y_pred: predicted values (batch_size x data_dimension)\n",
        "        device: device specification for pytorch ('cuda' / 'cpu)\n",
        "\n",
        "    Returns: The MSE loss between y_true and y_pred\n",
        "\n",
        "    \"\"\"\n",
        "    ### Your code starts here ###\n",
        "    import torch.nn.functional as F\n",
        "    y_true = torch.tensor(y_true, device=device)\n",
        "    y_pred = torch.tensor(y_pred, device=device)\n",
        "\n",
        "    loss = F.mse_loss(y_true, y_pred)\n",
        "    return loss\n",
        "    ### Your code ends here ###\n",
        "\n",
        "\n",
        "def prepare_dataset(dataset):\n",
        "    \"\"\"\n",
        "    Prepare the dataset for training and validation.\n",
        "\n",
        "    This function processes the dataset to create inputs and outputs for the dynamics and reward models.\n",
        "    It efficiently concatenates observations and actions, and computes the required labels.\n",
        "\n",
        "    Args:\n",
        "        dataset (list of tuples): The dataset containing observations, next observations, rewards, and actions.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Numpy arrays for X, y_reward, y_next_state, and y_diff.\n",
        "    \"\"\"\n",
        "    ### Your code starts here ###\n",
        "\n",
        "    # Unpack the dataset into separate lists\n",
        "    obs, next_obs, rewards, actions = zip(*dataset)\n",
        "\n",
        "    # Convert lists to numpy arrays for efficient operations\n",
        "    obs = np.array(obs)\n",
        "    next_obs = np.array(next_obs)\n",
        "    rewards = np.array(rewards)\n",
        "    actions = np.array(actions)\n",
        "\n",
        "    # Concatenate observations and actions\n",
        "    X = np.concatenate([obs, actions], axis=1)\n",
        "\n",
        "\n",
        "    # Compute y_diff\n",
        "    y_diff = next_obs - obs\n",
        "\n",
        "    ### Your code ends here ###\n",
        "    return X, rewards, next_obs, y_diff\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [],
      "source": [
        "def train_dynamics_models(dataset, env_model, rew_model, device):\n",
        "    \"\"\"\n",
        "    Train the two models that predict the next state and the expected reward\n",
        "    Args:\n",
        "        dataset: Random dataset to train the dynamics models on\n",
        "        env_model: Model that predicts the next state\n",
        "        rew_model: Model that predicts the reward\n",
        "        device:\n",
        "\n",
        "    Returns:\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    env_optimizer = optim.Adam(env_model.parameters(), lr=ENV_LEARNING_RATE)\n",
        "    rew_optimizer = optim.Adam(rew_model.parameters(), lr=REW_LEARNING_RATE)\n",
        "\n",
        "    # Split the dataset into randomly assigned train(80%) and test(20%) splits\n",
        "    ### Your code starts here ###\n",
        "    train_length = len(dataset) * 0.8\n",
        "    valid_length = len(dataset) - train_length\n",
        "\n",
        "    D_train, D_valid = torch.utils.data.random_split(dataset, [train_length, valid_length])\n",
        "    ### Your code ends here ###\n",
        "\n",
        "    X_train, y_rew_train, y_next_state_train, y_diff_train = prepare_dataset(D_train)\n",
        "    X_valid, y_rew_valid, y_next_state_valid, y_diff_valid = prepare_dataset(D_valid)\n",
        "\n",
        "    ### Normalize the inputs and outputs for nicer NN training ###\n",
        "\n",
        "\n",
        "    input_scaler = StandardScaler()\n",
        "    env_output_scaler = StandardScaler()\n",
        "    rew_output_scaler = StandardScaler()\n",
        "\n",
        "    # Normalize the features X, y_diff, y_rew by removing the mean and scaling to unit variance, i.e., normalizing to N(0,1).\n",
        "    # Fit the scalers on the training data and use them to transform the validation data and the planner in the main training loop.\n",
        "    ### Your code starts here ###\n",
        "    X_train = input_scaler.fit_transform(X_train)\n",
        "    y_diff_train = env_output_scaler.fit_transform(y_diff_train)\n",
        "    y_rew_train = rew_output_scaler.fit_transform(y_rew_train)\n",
        "\n",
        "    X_valid = input_scaler.transform(X_valid)\n",
        "    y_diff_valid = env_output_scaler.transform(y_diff_valid)\n",
        "    y_rew_valid = rew_output_scaler.transform(y_rew_valid)\n",
        "    ### Your code end here ###\n",
        "\n",
        "    # store all the scalers/normalizers in a list for later planning\n",
        "    normalizers = (input_scaler, env_output_scaler, rew_output_scaler)\n",
        "\n",
        "    losses_env = []\n",
        "    losses_rew = []\n",
        "    valid_env_loss = 0\n",
        "    valid_rew_loss = 0\n",
        "\n",
        "    # go through max_model_iter supervised iterations\n",
        "    for iteration in tqdm.tqdm(range(TRAIN_ITER_MODEL)):\n",
        "        # create mini batches of size batch_size\n",
        "        for batch in range(0, len(X_train), BATCH_SIZE):\n",
        "            if len(X_train) > batch + BATCH_SIZE:  # if there are enough examples left\n",
        "                X_batched = X_train[batch:batch + BATCH_SIZE]\n",
        "\n",
        "                y_diff_batched = y_diff_train[batch:batch + BATCH_SIZE]\n",
        "                y_rew_batched = y_rew_train[batch:batch + BATCH_SIZE]\n",
        "\n",
        "                # Add gaussian noise with mean 0 and variance 0.0001 as in the paper\n",
        "                X_batched += np.random.normal(loc=0, scale=0.001, size=X_batched.shape)\n",
        "\n",
        "                # Optimization of the 'env_model' and 'rew_model' networks\n",
        "                env_optimizer.zero_grad()\n",
        "                rew_optimizer.zero_grad()\n",
        "\n",
        "                # forward pass of the models to compute current outputs\n",
        "                pred_state = env_model(torch.tensor(X_batched).to(device))\n",
        "                pred_rew = rew_model(torch.tensor(X_batched).to(device))\n",
        "\n",
        "                # compute the MSE loss\n",
        "                loss_env = model_mse(y_diff_batched, pred_state, device)\n",
        "                loss_rew = model_mse(y_rew_batched, pred_rew, device)\n",
        "\n",
        "                # backward pass and optimization step\n",
        "                loss_env.backward()\n",
        "                env_optimizer.step()\n",
        "                loss_rew.backward()\n",
        "                rew_optimizer.step()\n",
        "\n",
        "                loss_env = loss_env.cpu().detach().numpy()\n",
        "                loss_rew = loss_rew.cpu().detach().numpy()\n",
        "                if iteration == (TRAIN_ITER_MODEL - 1):\n",
        "                    # take losses of the last iteration\n",
        "                    losses_env.append(loss_env)\n",
        "                    losses_rew.append(loss_rew)\n",
        "        if iteration % 10 == 0 or iteration == TRAIN_ITER_MODEL - 1:\n",
        "            # Evalute the models every 10 iterations and print the losses\n",
        "            env_model.eval()\n",
        "            rew_model.eval()\n",
        "\n",
        "            pred_state = env_model(torch.tensor(X_valid).to(device))\n",
        "            pred_rew = rew_model(torch.tensor(X_valid).to(device))\n",
        "            valid_env_loss = model_mse(y_diff_valid, pred_state, device).cpu().detach().numpy()\n",
        "            valid_rew_loss = model_mse(y_rew_valid, pred_rew, device).cpu().detach().numpy()\n",
        "\n",
        "            print(f\" Model Training Iteration: {iteration}, \"\n",
        "                  f\"Validation Dynamics Loss: {valid_env_loss}, \"\n",
        "                  f\"Validation Reward Loss: {valid_rew_loss}\")\n",
        "\n",
        "            # Set the models back to training mode\n",
        "            env_model.train(True)\n",
        "            rew_model.train(True)\n",
        "\n",
        "    losses_env = np.mean(losses_env)\n",
        "    losses_rew = np.mean(losses_rew)\n",
        "    return losses_env, losses_rew, valid_env_loss, valid_rew_loss, normalizers"
      ],
      "metadata": {
        "id": "wxIGlxTfrZwC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLORQjUQZVYb"
      },
      "source": [
        "# Planning With Dynamics And Reward Models\n",
        "\n",
        "Given the learned dynamics and reward models, we now want to select and execute actions that minimize a cost function (long term rewards). Ideally, you would calculate these actions by solving the following optimization:\n",
        "\n",
        "$$\n",
        "\\mathbf{a}_{t}^{*}=\\arg \\max _{\\mathbf{a}_{t: \\infty}} \\sum_{t^{\\prime}=t}^{\\infty} r_{\\theta}\\left(\\hat{\\mathbf{s}}_{t^{\\prime}}, \\mathbf{a}_{t^{\\prime}}\\right) \\text { where } \\hat{\\mathbf{s}}_{t^{\\prime}+1}=\\hat{\\mathbf{s}}_{t^{\\prime}}+f_{\\theta}\\left(\\hat{\\mathbf{s}}_{t^{\\prime}}, \\mathbf{a}_{t^{\\prime}}\\right)\n",
        "$$\n",
        "\n",
        "However, solving the above equation is impractical for two reasons:\n",
        "- planning over an infinite sequence of actions is impossible and\n",
        "- the learned dynamics model is imperfect, so using it to plan in such an open-loop manner will lead to accumulating errors over time and planning far into the future will become very inaccurate.\n",
        "\n",
        "Instead, we will solve the following gradient-free optimization problem:\n",
        "$$\n",
        "\\mathbf{A}^{*}=\\arg \\max _{\\left\\{\\mathbf{A}^{(0)}, \\ldots, \\mathbf{A}^{(K-1)}\\right\\}} \\sum_{t^{\\prime}=t}^{t+H-1} r_{\\theta}\\left(\\hat{\\mathbf{s}}_{t^{\\prime}}, \\mathbf{a}_{t^{\\prime}}\\right) \\text { s.t. } \\hat{\\mathbf{s}}_{t^{\\prime}+1}=\\hat{\\mathbf{s}}_{t^{\\prime}}+f_{\\theta}\\left(\\hat{\\mathbf{s}}_{t^{\\prime}}, \\mathbf{a}_{t^{\\prime}}\\right)\n",
        "$$\n",
        "in which $\\mathbf{A}^{(k)}=\\left(a_{t}^{(k)}, \\ldots, a_{t+H-1}^{(k)}\\right)$ are each a random action sequence of length $H$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Planner 1: Random Shooting Based Planner\n",
        "\n",
        "We will now use a simple random shooting method to solve the following gradient-free optimization problem :\n",
        "$$\n",
        "\\mathbf{A}^{*}=\\arg \\min _{\\left\\{\\mathbf{A}^{(0)}, \\ldots, \\mathbf{A}^{(K-1)}\\right\\}} \\sum_{t^{\\prime}=t}^{t+H-1} r_{\\theta}\\left(\\hat{\\mathbf{s}}_{t^{\\prime}}, \\mathbf{a}_{t^{\\prime}}\\right) \\text { s.t. } \\hat{\\mathbf{s}}_{t^{\\prime}+1}=\\hat{\\mathbf{s}}_{t^{\\prime}}+f_{\\theta}\\left(\\hat{\\mathbf{s}}_{t^{\\prime}}, \\mathbf{a}_{t^{\\prime}}\\right)\n",
        "$$\n",
        "in which $\\mathbf{A}^{(k)}=\\left(a_{t}^{(k)}, \\ldots, a_{t+H-1}^{(k)}\\right)$ are each a random action sequence of length $H$.\n",
        "\n",
        "**Random Shooting**: The simplest gradient-free optimizer simply generates $N$ independent random action sequences $\\left\\{A_{0} \\ldots A_{N}\\right\\}$, where each sequence $A_{i}=\\left\\{a_{t}^{i} \\ldots a_{t+H-1}^{i}\\right\\}$ is of length $H$ action. Given a reward function $r(s, a)$ that defines the task, and given future state predictions $\\hat{s}_{t+1}=f_{\\theta}\\left(\\hat{s}_{t}, a_{t}\\right)+\\hat{s}_{t}$ from the learned dynamics model $f_{\\theta}$, the optimal action sequence $A_{i^{*}}$ is selected to be the one corresponding to the sequence with highest predicted return: $i^{*}=\\arg \\max _{i} \\sum_{t^{\\prime}=t}^{t+H-1} r\\left(\\hat{s}_{t^{\\prime}}, a_{t^{\\prime}}^{i}\\right) .$"
      ],
      "metadata": {
        "collapsed": false,
        "id": "0hQPR4cBrZwC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "k465rsv0rL1X"
      },
      "outputs": [],
      "source": [
        "def random_control(env_model, rew_model, observation, sample_action, normalizers,\n",
        "                   device):\n",
        "    \"\"\"\n",
        "    Use a random-sampling method to generate action sequences.\n",
        "    The action for the first step that leads to the highest predicted sequence reward is returned.\n",
        "    Args:\n",
        "        env_model: The transition model\n",
        "        rew_model: The reward model\n",
        "        observation: The observation last seen from the real physical environment\n",
        "        sample_action: The function which is used to sample the action. Usually env.action_space.sample.\n",
        "        normalizers: Normalizers for the input and output of the models\n",
        "        device: The device to run the models on ('cuda' / 'cpu')\n",
        "\n",
        "    Returns: The action to take and the predicted sum of rewards\n",
        "\n",
        "    \"\"\"\n",
        "    # Prepare the models for evaluation\n",
        "    env_model.eval()\n",
        "    rew_model.eval()\n",
        "\n",
        "    input_scaler, env_output_scaler, rew_output_scaler = normalizers\n",
        "\n",
        "    # Initialize an array with repeated observations for batch processing\n",
        "    batch_obs = np.array([observation for _ in range(NUM_ACTIONS_SEQUENCES)])\n",
        "\n",
        "    # Array to store cumulative rewards for all action sequences\n",
        "    cumulative_rewards = np.zeros((NUM_ACTIONS_SEQUENCES, 1))\n",
        "\n",
        "    # Sampling action sequences for each trajectory\n",
        "    sampled_action_sequences = []\n",
        "    for _ in range(NUM_ACTIONS_SEQUENCES):\n",
        "        sampled_action_sequence = [sample_action() for _ in range(HORIZON_LENGTH)]\n",
        "        sampled_action_sequences.append(sampled_action_sequence)\n",
        "    sampled_action_sequences = np.array(sampled_action_sequences)\n",
        "\n",
        "    for t in range(HORIZON_LENGTH):\n",
        "        # select action for time step t for each sequence\n",
        "        sampled_actions = sampled_action_sequences[:, t, :]\n",
        "\n",
        "        # scale the input\n",
        "        models_input = input_scaler.transform(np.concatenate([batch_obs, sampled_actions], axis=1))\n",
        "        # compute the next state and reward for the state-action pair\n",
        "        pred_obs = env_model(torch.tensor(models_input).to(device))\n",
        "        pred_rew = rew_model(torch.tensor(models_input).to(device))\n",
        "\n",
        "        # unnormalize/descale and add previous observation\n",
        "        pred_obs = env_output_scaler.inverse_transform(pred_obs.cpu().detach().numpy())\n",
        "        batch_obs = pred_obs + batch_obs\n",
        "\n",
        "        # sum of the expected rewards\n",
        "        cumulative_rewards += pred_rew.cpu().detach().numpy()\n",
        "\n",
        "    # Identify the sequence with the highest cumulative reward\n",
        "    arg_best_reward = np.argmax(cumulative_rewards)\n",
        "    best_sum_reward = cumulative_rewards[arg_best_reward].squeeze()\n",
        "    # take the first action of this sequence\n",
        "    first_actions = sampled_action_sequences[:, 0, :]\n",
        "    best_action = first_actions[arg_best_reward]\n",
        "\n",
        "    env_model.train(True)\n",
        "    rew_model.train(True)\n",
        "    return best_action, best_sum_reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfd3KIAlZVYb"
      },
      "source": [
        "# Cross Entropy Method (CEM) for Planning\n",
        "The random shooting approach has been shown to achieve success on continuous control tasks with learned models, but it has numerous drawbacks: it scales poorly with the dimension of both the planning horizon and the action space, and it often is insufficient for achieving high task performance since a sequence of actions sampled at random often does not directly lead to meaningful behavior. Therefore, the Cross Entropy Method is a favorable planning approach in model based reinforcement learning.\n",
        "\n",
        "We already got to know to CEM in the stochastic search homework. However, here, we consider a different application.\n",
        "\n",
        "We will use CEM to solve the following gradient-free optimization problem :\n",
        "$$\n",
        "\\mathbf{A}^{*}=\\arg \\min _{\\left\\{\\mathbf{A}^{(0)}, \\ldots, \\mathbf{A}^{(K-1)}\\right\\}} \\sum_{t^{\\prime}=t}^{t+H-1} r_{\\theta}\\left(\\hat{\\mathbf{s}}_{t^{\\prime}}, \\mathbf{a}_{t^{\\prime}}\\right) \\text { s.t. } \\hat{\\mathbf{s}}_{t^{\\prime}+1}=\\hat{\\mathbf{s}}_{t^{\\prime}}+f_{\\theta}\\left(\\hat{\\mathbf{s}}_{t^{\\prime}}, \\mathbf{a}_{t^{\\prime}}\\right)\n",
        "$$\n",
        "in which $\\mathbf{A}^{(k)}=\\left(a_{t}^{(k)}, \\ldots, a_{t+H-1}^{(k)}\\right)$ are each a random action sequence of length $H$\n",
        "\n",
        "**Cross-Entropy Method** (`CEM`) is an optimization algorithm applicable to problems that are both **combinatorial** and **continuous**, which is our case: find the best performing sequence of actions.\n",
        "\n",
        "The Cross-entropy method (CEM) approach, begins like the random shooting approach, but then does this sampling for multiple iterations $m \\in\\{0 \\ldots M\\}$ at each time step. The top $J$ highest-scoring action sequences from each iteration are used to update and refine the mean and variance of the sampling distribution for the next iteration, as follows:\n",
        "\n",
        "\\begin{aligned}\n",
        "A_{i} &=\\left\\{a_{0}^{i} \\ldots a_{H-1}^{i}\\right\\} \\text {, where } a_{t}^{i} \\sim \\mathcal{N}\\left(\\mu_{t}^{m}, \\Sigma_{t}^{m}\\right) \\forall i \\in N, t \\in 0 \\ldots H-1 \\\\\n",
        "A_{\\text {elites }} &=\\operatorname{sort}\\left(A_{i}\\right)[-J:] \\\\\n",
        "\\mu_{t}^{m+1} &=  \\operatorname{mean}\\left(A_{\\text {elites }}\\right) \\quad \\forall t \\in 0 \\ldots H-1 \\\\\n",
        "\\Sigma_{t}^{m+1} &= \\operatorname{var}\\left(A_{\\text {elites }}\\right)\\quad  \\forall t \\in 0 \\ldots H-1\n",
        "\\end{aligned}\n",
        "\n",
        "After $M$ iterations, the optimal actions are selected to be the resulting mean of the action distribution.\n",
        "Note that, since our model is imperfect and things will never go perfectly according to plan, we adopt a model predictive control (MPC) approach.\n",
        "The MPC planner replans at every time step similar to previous section with random shooting planner. Students may refer to [this paper](/https://arxiv.org/pdf/1909.11652.pdf).\n",
        "\n",
        "## Task 2: Implementing the CEM for Planning (7 Pts)\n",
        "In the following task you will implement the CEM following the standard procedure (5 Pts). In each optimization step, you need to\n",
        "- sample a sequence of actions $A_{i}$ from the Gaussian distribution from the iteration before\n",
        "- prepare the data for inputting to the dynamics and rewards model by standardizing it\n",
        "- perform the inference of the reward model to obtain a reward prediction\n",
        "- perform the inference via the dynamics model to obtain the observation difference prediction\n",
        "- unnormalize the predicted observation difference\n",
        "- obtain the observation of the next step by adding the predicted observation to the current observation\n",
        "- sum the predicted reward to the rewards from the steps before\n",
        "\n",
        "After each optimization step (2 Pts):\n",
        "- obtain the top 10 action sequences $A_{i}$  which lead to the highest cumulative reward\n",
        "- update the search distribution by calculating the sample mean and the sample standard deviation using the elite samples\n",
        "**NOTE:** We only consider the standard deviation of each dimension of the search space, i.e., we use an isotropic Gaussian distribution.\n",
        "\n",
        "We have provided you with comments guiding you through the parts you need to implement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rgiU2d1OrL1X"
      },
      "outputs": [],
      "source": [
        "def cem_control(env_model, rew_model, observation, sample_action, normalizers,\n",
        "                device):\n",
        "    \"\"\"\n",
        "    Use a cross entropy method to generate action sequences.\n",
        "    The action for the first step that leads to the highest predicted sequence reward is returned.\n",
        "    Args:\n",
        "        env_model: The transition model\n",
        "        rew_model: The reward model\n",
        "        observation: The observation last seen from the real physical environment\n",
        "        sample_action: The function which is used to sample the action. Usually env.action_space.sample.\n",
        "        normalizers: Normalizers for the input and output of the models\n",
        "        device: The device to run the models on ('cuda' / 'cpu')\n",
        "\n",
        "    Returns: The action to take and the predicted sum of rewards\n",
        "\n",
        "    \"\"\"\n",
        "    env_model.eval()\n",
        "    rew_model.eval()\n",
        "\n",
        "    input_scaler, env_output_scaler, rew_output_scaler = normalizers\n",
        "\n",
        "    # Batch-wise sampling of action sequences. Each sequence is of length H.\n",
        "    sampled_action_sequences = []\n",
        "    for _ in range(NUM_ACTIONS_SEQUENCES):\n",
        "        sampled_action_sequence = [sample_action() for _ in range(HORIZON_LENGTH)]\n",
        "        sampled_action_sequences.append(sampled_action_sequence)\n",
        "    sampled_action_sequences = np.array(sampled_action_sequences)\n",
        "    action_dim = sampled_action_sequences.shape[-1]\n",
        "\n",
        "    ### Your code starts here ###\n",
        "\n",
        "    # Calculate the mean and standard deviation of the sampled action sequences to get an initial estimate of the\n",
        "    # search distribution\n",
        "    action_mean = ...\n",
        "    action_std_dev = ...\n",
        "\n",
        "    first_sampled_actions = []\n",
        "    cumulative_rewards = None\n",
        "    for _ in range(CEM_OPTIMIZATION_ITERS):\n",
        "        \"\"\"\n",
        "        In this section you implement an MPC with CEM method.  In each optimization iteration,\n",
        "        you sample actions sequences from a normal distribution, Calculate the cost for each\n",
        "        sequence using the learned dynamics and reward models. We will use MPC similar to the\n",
        "        previous section to re-plan at every timestep.\n",
        "        \"\"\"\n",
        "        # Sample from a Gaussian Using The Means and Standard Deviations\n",
        "        sampled_action_sequences = ...\n",
        "\n",
        "        # Initialize an array with repeated observations for batch processing\n",
        "        cumulative_rewards = np.zeros((NUM_ACTIONS_SEQUENCES, 1))\n",
        "        batch_obs = ...\n",
        "\n",
        "        first_sampled_actions = sampled_action_sequences[:, 0, :]\n",
        "        for t in range(HORIZON_LENGTH):\n",
        "            # sample actions for each sequence and scale the input\n",
        "            sampled_actions = ...\n",
        "            models_input = input_scaler.transform(np.concatenate([batch_obs, sampled_actions], axis=1))\n",
        "\n",
        "            # compute the differences to the next state using the dynamics and reward model\n",
        "            pred_obs = ...\n",
        "            pred_rew = ...\n",
        "\n",
        "            # unnormalize/descale and add previous observation\n",
        "            pred_obs_unnormalized = ...\n",
        "            batch_obs = pred_obs_unnormalized + batch_obs\n",
        "\n",
        "            # sum the expected rewards\n",
        "            cumulative_rewards += pred_rew.cpu().detach().numpy()\n",
        "\n",
        "        # Select Top K Action Sequences (lets call them elite_sequences) that gave the highest cumulative reward\n",
        "        elite_sequences = ...\n",
        "\n",
        "        # Update the mean and variances of the search distribution using the elite (topk) action sequences\n",
        "        action_mean, action_std_dev = ...\n",
        "\n",
        "    ### Your code ends here ###\n",
        "\n",
        "    # Pick the first action of the sequence with the highest cumulative reward\n",
        "    arg_best_reward = np.argmax(cumulative_rewards)\n",
        "    best_sum_reward = cumulative_rewards[arg_best_reward].squeeze()\n",
        "\n",
        "    # take the first action of this sequence\n",
        "    best_action = first_sampled_actions[arg_best_reward]\n",
        "    # you can also choose the best action as the mean of the elites from last optimization iteration.\n",
        "    # However, here we do a greedy step in the end where the best among the elites from last iteration is chosen.\n",
        "\n",
        "    env_model.train(True)\n",
        "    rew_model.train(True)\n",
        "\n",
        "    return best_action, best_sum_reward"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3: What is a Model Predictive Controller (MPC)? (1 Pts)\n",
        "\n",
        "Since our dynamics and reward models can be imperfect and things will never go perfectly ac-ording to plan, we adopt a model predictive control (MPC) approach. Explain in few lines whats the basic priniciple behind an MPC."
      ],
      "metadata": {
        "collapsed": false,
        "id": "HK-wZ5jLrZwD"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCwXxx7VZVYc"
      },
      "source": [
        "# Main Loop (2 Pts)\n",
        "\n",
        "Start with the default hyperparameters and run the main MBRL loop with the two planners. Try out other hyperparameters and compare the reward plots of both planners for these parameters. Briefly (2-3 sentences) describe your observations.\n",
        "\n",
        "\\begin{aligned}\n",
        "&\\overline{\\text { Algorithm } \\mathbf{1} \\text { Model-based Reinforcement Learning }} \\\\\n",
        "&\\hline \\text { 1: gather dataset } \\mathcal{D}_{\\text {RAND }} \\text { of random trajectories } \\\\\n",
        "&\\text { 2: initialize empty dataset } \\mathcal{D}_{\\text {RL }} \\text {, and randomly initialize } f_{\\theta} \\\\\n",
        "&  \\text{ 3: } \\textbf{for} \\text{ iter=1 to max_iter } \\textbf{do}  \\\\\n",
        "&\\quad  \\quad \\quad \\textbf {Model Learning } \\\\\n",
        "&\\text { 4:} \\quad  \\quad \\text{ train } f_{\\theta}(\\mathbf{s}, \\mathbf{a}) \\text{ and } r_{\\theta}(\\mathbf{s}, \\mathbf{a}) \\text { by performing gradient descent on MSE Loss }  \\\\\n",
        "& \\quad \\quad  \\quad \\text { using } \\mathcal{D}_{\\text {RAND }} \\text { and } \\mathcal{D}_{\\text {RL }} \\\\\n",
        "&\\quad  \\quad \\quad \\textbf {Planning and More Experience Collection } \\\\\n",
        "&\\text { 5: } \\quad  \\quad  \\textbf { for } t=1 \\text { to } T \\textbf { do } \\\\\n",
        "&\\text { 6: } \\quad \\quad \\quad \\quad \\text { get agent's current state } \\mathbf{s}_{t} \\\\\n",
        "&\\text { 7: } \\quad \\quad \\quad \\quad \\text { use } f_{\\theta} \\text { and } r_{\\theta} \\text { to estimate optimal action sequence } \\mathbf{A}_{t}^{(H)} \\\\\n",
        "&\\text { 8: } \\quad \\quad \\quad \\quad \\text { execute first action } \\mathbf{a}_{t} \\text { from selected action sequence } \\\\\n",
        "&\\text { 9: } \\quad \\quad \\quad \\quad\\text { Add } \\{s_t,  s_{t+1}, r_t, a_t\\} \\text { to } \\mathcal{D}_{\\text {RL }} \\\\\n",
        "&\\text { 10: } \\quad \\text { end for } \\\\\n",
        "&\\text { 11: end for } \\\\\n",
        "&\\hline\n",
        "\\end{aligned}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_qQb789_syt0"
      },
      "outputs": [],
      "source": [
        "def train():\n",
        "    device = 'cpu'  # 'cuda' or 'cpu'\n",
        "    environment_str = \"Pendulum-v1\"  # \"InvertedPendulum\"\n",
        "\n",
        "    video_folder = DATA_ROOT / \"exercise_4\" / time.strftime(\"%Y-%m-%d_%H-%M\")\n",
        "\n",
        "    env = gym.make(environment_str, render_mode=\"rgb_array_list\")\n",
        "    _ = env.reset()\n",
        "\n",
        "    # Step 1:  gather the dataset of random sequences\n",
        "    dataset = gather_random_trajectories(env=env)  # initialize the dataset randomly\n",
        "\n",
        "    return_list = []\n",
        "    train_env_losses = []\n",
        "    train_rew_losses = []\n",
        "    validation_env_losses = []\n",
        "    validation_rew_losses = []\n",
        "\n",
        "    # Step 2: Initialize the models\n",
        "    in_features = env.unwrapped.action_space.shape[0] + env.unwrapped.observation_space.shape[0]\n",
        "    env_model = FeedforwardModel(input_dim=in_features,\n",
        "                                 output_dim=env.unwrapped.observation_space.shape[0]).to(device)\n",
        "    rew_model = FeedforwardModel(input_dim=in_features,\n",
        "                                 output_dim=1).to(device)\n",
        "\n",
        "    # Step 3: Iterate over model learning, planning and on-policy experience collection\n",
        "    for n_iter in range(NUM_ITERATIONS):\n",
        "\n",
        "        # supervised training of the current experience dataset\n",
        "        train_env_loss, train_rew_loss, valid_env_loss, valid_rew_loss, standardizers = train_dynamics_models(dataset,\n",
        "                                                                                                              env_model,\n",
        "                                                                                                              rew_model,\n",
        "                                                                                                              device)\n",
        "        env = gym.make(environment_str, render_mode=\"rgb_array_list\")\n",
        "\n",
        "        returns = []\n",
        "\n",
        "        num_transitions = 0\n",
        "        p_bar = tqdm.tqdm(total=NUM_STEPS, desc=\"MPC Planner Steps: \")\n",
        "        while num_transitions < NUM_STEPS:\n",
        "            done = False\n",
        "            env_return = 0\n",
        "            pred_return = 0\n",
        "            obs, _ = env.reset()\n",
        "            while not done:\n",
        "\n",
        "                # Execute the control to roll the sequences and pick the first action of the sequence with the higher reward\n",
        "                planner_fn = cem_control if PLANNER == 'cem' else random_control\n",
        "                action, pred_rew = planner_fn(env_model, rew_model, obs,\n",
        "                                              env.action_space.sample,\n",
        "                                              standardizers, device)\n",
        "\n",
        "                # one step in the environment with the action returned by the controller\n",
        "                new_obs, reward, terminated, truncated, _ = env.step(action)  # Apply action\n",
        "                done = terminated or truncated\n",
        "\n",
        "                ## Compute the reward using the reward model\n",
        "                rew_model.eval()\n",
        "                input_scaler, _, rew_output_scaler = standardizers\n",
        "                models_input = input_scaler.transform([np.concatenate([obs, action])])\n",
        "                pred_reward = rew_model(torch.tensor(models_input).to(device))\n",
        "                unnorm_reward = rew_output_scaler.inverse_transform(pred_reward.cpu().detach().numpy())\n",
        "                pred_return += unnorm_reward\n",
        "                rew_model.train(True)\n",
        "\n",
        "                # add the transition to the dataset\n",
        "                dataset.append([obs, new_obs, reward, action])\n",
        "                obs = new_obs\n",
        "                env_return += reward\n",
        "                num_transitions += 1\n",
        "                p_bar.update(1)\n",
        "\n",
        "                # if the environment is done, print some stats\n",
        "                if done:\n",
        "                    returns.append(env_return)\n",
        "                    print(f\"Observed/predicted return for current episode: {env_return}/{np.mean(pred_return)}\")\n",
        "\n",
        "        print(f\"Average returns at iteration {n_iter}: {np.mean(returns)} \")\n",
        "        return_list.append(np.mean(returns))\n",
        "        train_env_losses.append(train_env_loss)\n",
        "        validation_env_losses.append(valid_env_loss)\n",
        "        train_rew_losses.append(train_rew_loss)\n",
        "        validation_rew_losses.append(valid_rew_loss)\n",
        "\n",
        "        # clean up and save video of last episode\n",
        "        save_video(\n",
        "            env.render(),\n",
        "            video_folder=video_folder,\n",
        "            episode_trigger=lambda x: x % 250 == 0,\n",
        "            name_prefix=\"mbrl\",\n",
        "            episode_index=NUM_ITERATIONS,\n",
        "            fps=30,\n",
        "        )\n",
        "        env.close()\n",
        "        p_bar.close()\n",
        "\n",
        "    # Step 4: Do some plotting\n",
        "    plot_rewards(np.array(return_list),\n",
        "                 colors=[\"blue\"],\n",
        "                 labels=[\"MBRL\"],\n",
        "                 title=f\"MBRL - {PLANNER}\",\n",
        "                 ylabel=\"Return\")\n",
        "    save_current_figure(f\"MPC_Reward_{PLANNER}\")\n",
        "\n",
        "    plot_rewards(np.array(train_env_losses), np.array(validation_env_losses),\n",
        "                 colors=[\"blue\", \"red\"],\n",
        "                 labels=[\"Train\", \"Validation\"],\n",
        "                 title=f\"MBRL - {PLANNER}\",\n",
        "                 ylabel=\"Env Loss\")\n",
        "    save_current_figure(f\"Env_Loss_{PLANNER}\")\n",
        "\n",
        "    plot_rewards(np.array(train_rew_losses), np.array(validation_rew_losses),\n",
        "                 colors=[\"blue\", \"red\"],\n",
        "                 labels=[\"Train\", \"Validation\"],\n",
        "                 title=f\"MBRL - {PLANNER}\",\n",
        "                 ylabel=\"Reward Loss\")\n",
        "    save_current_figure(f\"Rew_Loss_{PLANNER}\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "train()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "eQx7oDGeeKWj"
      ],
      "name": "2_dqn_atari.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}